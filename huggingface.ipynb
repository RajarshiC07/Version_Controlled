{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# from langchain_community.llms import HuggingFaceHub\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceHub\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Op\n\u001b[0;32m      5\u001b[0m ENDPOINT_URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-xxl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
     ]
    }
   ],
   "source": [
    "# from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "from langchain.llms import HuggingFaceHub\n",
    "ENDPOINT_URL = \"google/flan-t5-xxl\"\n",
    "\n",
    "LLM = HuggingFaceHub(repo_id = ENDPOINT_URL, huggingfacehub_api_token = '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', model_kwargs={\"temperature\":1})\n",
    "# python -c \"from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The American'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAME = LLM(\"I want to open an American restaurant. What should be it's name?\")\n",
    "NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Suggest a fancy name for my restaurant that serves Russian food'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "my_prompt_template = PromptTemplate(input_variables=['cuisine'], template=\"Suggest a fancy name for my restaurant that serves {cuisine} food\")\n",
    "my_prompt_template.format(cuisine = \"Russian\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Indian', 'restaurant_name': 'sai sai'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "llmchain = LLMChain(llm = LLM, prompt = my_prompt_template, tags=[\"detailed\"], output_key='restaurant_name')\n",
    "llmchain.invoke(\"Indian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "menu_template = PromptTemplate(input_variables=['restaurant_name'], template=\"Suggest list of veg menu items for my {restaurant_name} restaurant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'restaurant_name': 'sai sai', 'menu': 'Vegetable naan, sai sai na'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llmmenuchain = LLMChain(llm = LLM, prompt = menu_template, tags=[\"detailed\"], output_key='menu')\n",
    "llmmenuchain.invoke(\"sai sai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "cultureTemplate = PromptTemplate(input_variables=['country'], template=\"Tell 1 historical fact about {country} culture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'country': 'India', 'culture_facts': 'The Indian subcontinent is a continent'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "culturechain = LLMChain(llm = LLM, prompt = cultureTemplate, tags=[\"detailed\"], output_key='culture_facts')\n",
    "culturechain.invoke(\"India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cuisine': 'Indian',\n",
       " 'country': 'India',\n",
       " 'restaurant_name': 'sai sai',\n",
       " 'menu': 'Vegetable naan, sai sai na',\n",
       " 'culture_facts': 'The Indian subcontinent is a continent'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "parentChain = SequentialChain(chains = [llmchain, llmmenuchain, culturechain], input_variables=['cuisine', 'country'], output_variables=['restaurant_name','menu','culture_facts'])\n",
    "\n",
    "parentChain.invoke({\"cuisine\":\"Indian\", \"country\":\"India\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Big\",\n",
    "        \"answer\": \"Small\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"hot\",\n",
    "        \"answer\": \"cold\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"day\",\n",
    "        \"answer\": \"night\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"good\",\n",
    "        \"answer\": \"bad\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Big\n",
      "Small\n"
     ]
    }
   ],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\",\"answer\"], template=\"Question: {question}\\n{answer}\"\n",
    ")\n",
    "\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Big\n",
      "Small\n",
      "\n",
      "Question: hot\n",
      "cold\n",
      "\n",
      "Question: day\n",
      "night\n",
      "\n",
      "Question: good\n",
      "bad\n",
      "\n",
      "Question: hot\n"
     ]
    }
   ],
   "source": [
    "prompt_sample = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "print(prompt_sample.format(input=\"hot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'Question: day', 'antonym': 'a day of fun'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fewshotchain= LLMChain(llm = LLM, prompt = prompt_sample, output_key='antonym')\n",
    "fewshotchain.invoke(\"Question: day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'India is a country in the Indian subcontinent.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Tell 3 key points about India?\"\n",
    "result = LLM.predict(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "GoogleAuthError",
     "evalue": "\nUnable to authenticate your request.\nDepending on your runtime environment, you can complete authentication by:\n- if in local JupyterLab instance: `!gcloud auth login` \n- if in Colab:\n    -`from google.colab import auth`\n    -`auth.authenticate_user()`\n- if in service account or other: please follow guidance in https://cloud.google.com/docs/authentication",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\google\\cloud\\aiplatform\\initializer.py:302\u001b[0m, in \u001b[0;36m_Config.project\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 302\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_project_as_env_var_or_google_auth_default()\n\u001b[0;32m    303\u001b[0m     project_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_project\n",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\google\\cloud\\aiplatform\\initializer.py:93\u001b[0m, in \u001b[0;36m_Config._set_project_as_env_var_or_google_auth_default\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 93\u001b[0m     credentials, project \u001b[38;5;241m=\u001b[39m google\u001b[38;5;241m.\u001b[39mauth\u001b[38;5;241m.\u001b[39mdefault()\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credentials \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_credentials \u001b[38;5;129;01mor\u001b[39;00m credentials\n",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\google\\auth\\_default.py:691\u001b[0m, in \u001b[0;36mdefault\u001b[1;34m(scopes, request, quota_project_id, default_scopes)\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m credentials, effective_project_id\n\u001b[1;32m--> 691\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mDefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "\u001b[1;31mDefaultCredentialsError\u001b[0m: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGoogleAuthError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\vertexai\\_model_garden\\_model_garden_models.py:291\u001b[0m, in \u001b[0;36m_ModelGardenModel.from_pretrained\u001b[1;34m(cls, model_name)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_pretrained(interface_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, model_name\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m auth_exceptions\u001b[38;5;241m.\u001b[39mGoogleAuthError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\vertexai\\_model_garden\\_model_garden_models.py:208\u001b[0m, in \u001b[0;36m_from_pretrained\u001b[1;34m(interface_class, model_name, publisher_model, tuned_vertex_model)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    205\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minterface_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a correct model interface class since it does not have an instance schema URI.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m         )\n\u001b[1;32m--> 208\u001b[0m     model_info \u001b[38;5;241m=\u001b[39m _get_model_info(\n\u001b[0;32m    209\u001b[0m         model_id\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[0;32m    210\u001b[0m         schema_to_class_map\u001b[38;5;241m=\u001b[39m{interface_class\u001b[38;5;241m.\u001b[39m_INSTANCE_SCHEMA_URI: interface_class},\n\u001b[0;32m    211\u001b[0m     )\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\vertexai\\_model_garden\\_model_garden_models.py:124\u001b[0m, in \u001b[0;36m_get_model_info\u001b[1;34m(model_id, schema_to_class_map, interface_class, publisher_model_res, tuned_vertex_model)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m publisher_model_res:\n\u001b[0;32m    123\u001b[0m     publisher_model_res \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 124\u001b[0m         _publisher_models\u001b[38;5;241m.\u001b[39m_PublisherModel(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    125\u001b[0m             resource_name\u001b[38;5;241m=\u001b[39mmodel_id\n\u001b[0;32m    126\u001b[0m         )\u001b[38;5;241m.\u001b[39m_gca_resource\n\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m publisher_model_res\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublishers/google/models/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\google\\cloud\\aiplatform\\_publisher_models.py:63\u001b[0m, in \u001b[0;36m_PublisherModel.__init__\u001b[1;34m(self, resource_name, project, location, credentials)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves an existing PublisherModel resource given a resource name or model garden id.\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m        Overrides credentials set in aiplatform.init.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(project\u001b[38;5;241m=\u001b[39mproject, location\u001b[38;5;241m=\u001b[39mlocation, credentials\u001b[38;5;241m=\u001b[39mcredentials)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_resource_name(resource_name):\n",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\google\\cloud\\aiplatform\\base.py:556\u001b[0m, in \u001b[0;36mVertexAiResourceNoun.__init__\u001b[1;34m(self, project, location, credentials, resource_name)\u001b[0m\n\u001b[0;32m    552\u001b[0m     project, location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_and_validate_project_location(\n\u001b[0;32m    553\u001b[0m         resource_name\u001b[38;5;241m=\u001b[39mresource_name, project\u001b[38;5;241m=\u001b[39mproject, location\u001b[38;5;241m=\u001b[39mlocation\n\u001b[0;32m    554\u001b[0m     )\n\u001b[1;32m--> 556\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproject \u001b[38;5;241m=\u001b[39m project \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mproject\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation \u001b[38;5;241m=\u001b[39m location \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mlocation\n",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\google\\cloud\\aiplatform\\initializer.py:305\u001b[0m, in \u001b[0;36m_Config.project\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GoogleAuthError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GoogleAuthError(project_not_found_exception_str) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m project_id:\n",
      "\u001b[1;31mGoogleAuthError\u001b[0m: Unable to find your project. Please provide a project ID by:\n- Passing a constructor argument\n- Using vertexai.init()\n- Setting project using 'gcloud config set project my-project'\n- Setting a GCP environment variable\n- To create a Google Cloud project, please follow guidance at https://developers.google.com/workspace/guides/create-project",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mGoogleAuthError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatVertexAI\n\u001b[1;32m----> 2\u001b[0m chat \u001b[38;5;241m=\u001b[39m ChatVertexAI()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     AIMessage,\n\u001b[0;32m      5\u001b[0m     HumanMessage,\n\u001b[0;32m      6\u001b[0m     SystemMessage\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m chat([HumanMessage(content\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me a fun fact.\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     emit_warning()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\langchain_core\\load\\serializable.py:120\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:339\u001b[0m, in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\pydantic\\main.py:1102\u001b[0m, in \u001b[0;36mpydantic.main.validate_model\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\langchain_community\\chat_models\\vertexai.py:248\u001b[0m, in \u001b[0;36mChatVertexAI.validate_environment\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    246\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    247\u001b[0m         model_cls \u001b[38;5;241m=\u001b[39m ChatModel\n\u001b[1;32m--> 248\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model_cls\u001b[38;5;241m.\u001b[39mfrom_pretrained(values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[1;32mc:\\Users\\rajar\\anaconda3\\Lib\\site-packages\\vertexai\\_model_garden\\_model_garden_models.py:293\u001b[0m, in \u001b[0;36m_ModelGardenModel.from_pretrained\u001b[1;34m(cls, model_name)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _from_pretrained(interface_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, model_name\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[0;32m    292\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m auth_exceptions\u001b[38;5;241m.\u001b[39mGoogleAuthError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m auth_exceptions\u001b[38;5;241m.\u001b[39mGoogleAuthError(credential_exception_str) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mGoogleAuthError\u001b[0m: \nUnable to authenticate your request.\nDepending on your runtime environment, you can complete authentication by:\n- if in local JupyterLab instance: `!gcloud auth login` \n- if in Colab:\n    -`from google.colab import auth`\n    -`auth.authenticate_user()`\n- if in service account or other: please follow guidance in https://cloud.google.com/docs/authentication"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatHunyuan\n",
    "chat = ChatVertexAI()\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "chat([HumanMessage(content=\"Tell me a fun fact.\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 545 entries, 0 to 544\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   price             545 non-null    int64 \n",
      " 1   area              545 non-null    int64 \n",
      " 2   bedrooms          545 non-null    int64 \n",
      " 3   bathrooms         545 non-null    int64 \n",
      " 4   stories           545 non-null    int64 \n",
      " 5   mainroad          545 non-null    object\n",
      " 6   guestroom         545 non-null    object\n",
      " 7   basement          545 non-null    object\n",
      " 8   hotwaterheating   545 non-null    object\n",
      " 9   airconditioning   545 non-null    object\n",
      " 10  parking           545 non-null    int64 \n",
      " 11  prefarea          545 non-null    object\n",
      " 12  furnishingstatus  545 non-null    object\n",
      "dtypes: int64(6), object(7)\n",
      "memory usage: 55.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"D:\\Version_Controlled\\Code_Base\\Practice\\ML_Self_Prac\\Housing.csv\")\n",
    "df.info()\n",
    "# df.pop(\"furnishingstatus\")\n",
    "# df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
